在所有节点上进行如下操作：
 1. 设置主机名hostname，管理节点设置主机名为master
    hostnamectl set-hostname master
    hostnamectl set-hostname node1
    hostnamectl set-hostname node2
	需要设置其他主机名称时，可将 master 替换为正确的主机名node1、node2即可。
 2.编辑 /etc/hosts 文件，添加域名解析。
    cat <<EOF >>/etc/hosts
    10.10.10.10 master
    10.10.10.11 node1
    10.10.10.12 node2
    EOF
3.关闭防火墙、selinux和swap。
systemctl stop firewalld
systemctl disable firewalld

   setenforce 0

   sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config

   swapoff -a

   sed -i 's/.*swap.*/#&/' /etc/fstab
4.配置内核参数，将桥接的IPv4流量传递到iptables的链
   cat > /etc/sysctl.d/k8s.conf <<EOF
   net.bridge.bridge-nf-call-ip6tables = 1
   net.bridge.bridge-nf-call-iptables = 1
   EOF
   sysctl --system

5.配置国内yum源
  yum install -y wget

  mkdir /etc/yum.repos.d/bak && mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak

  wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.cloud.tencent.com/repo/centos7_base.repo

  wget -O /etc/yum.repos.d/epel.repo http://mirrors.cloud.tencent.com/repo/epel-7.repo

  yum clean all && yum makecache
  配置国内Kubernetes源
  cat <<EOF > /etc/yum.repos.d/kubernetes.repo

  [kubernetes]

  name=Kubernetes

  baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/

  enabled=1

  gpgcheck=1

  repo_gpgcheck=1

  gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg

EOF
 配置 docker 源
 wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo
6.安装docker
yum install -y docker-ce-18.06.1.ce-3.el7

systemctl enable docker && systemctl start docker
7.安装kubeadm、kubelet、kubectl
    yum install kubelet-1.14.2 kubeadm-1.14.2 kubectl-1.14.2 -y

  systemctl enable kubelet
  说明：
  Kubelet负责与其他节点集群通信，并进行本节点Pod和容器生命周期的管理。
  Kubeadm是Kubernetes的自动化部署工具，降低了部署难度，提高效率。
  Kubectl是Kubernetes集群管理工具。
  
  
在master上进行如下操作：
8.部署master节点
 8.1集群初始化

kubeadm init --kubernetes-version=1.14.2 \
--apiserver-advertise-address=172.16.16.5  \
--image-repository registry.aliyuncs.com/google_containers \
--service-cidr=10.1.0.0/16 \
--pod-network-cidr=10.244.0.0/16



说明：
定义POD的网段为: 10.244.0.0/16， api server地址就是master本机IP地址。
这一步很关键，由于kubeadm 默认从官网k8s.grc.io下载所需镜像，国内无法访问，
因此需要通过–image-repository指定阿里云镜像仓库地址，很多新手初次部署都卡在此环节无法进行后续配置。

9.初始化完成，记录join命令

  kubeadm join 114.67.80.169:6443 --token 6hycoj.mmxc3wwvckfip1k5 \
    --discovery-token-ca-cert-hash sha256:26a1987f705f6a1c12147ec2949c1ded70774c7833a379d24e77abcc54ea86cc 

10.配置kubectl工具
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

  kubectl get nodes

  查看集群状态：
  kubectl get cs
11. 部署flannel网络
  kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml

部署node节点
12.执行kubeadm join命令，使所有node节点加入集群

在master上进行如下操作：
13. 集群状态检测
 在master节点输入命令检查集群状态，返回如下结果则集群状态正常。
   kubectl get nodes
   重点查看STATUS内容为Ready时，则说明集群状态正常。

14.部署Dashboard
 14.1.创建Dashboard的yaml文件
 wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
 sed -i 's/k8s.gcr.io/loveone/g' kubernetes-dashboard.yaml

 sed -i '/targetPort:/a\ \ \ \ \ \ nodePort: 30001\n\ \ type: NodePort' kubernetes-dashboard.yaml
  14.2.部署Dashboard
  kubectl create -f kubernetes-dashboard.yaml
  14.3.创建完成后，检查相关服务运行状态
  kubectl get deployment kubernetes-dashboard -n kube-system

  kubectl get pods -n kube-system -o wide
  
  查看 Pod 信息
  kubectl get pod -A -o wide

  kubectl get services -n kube-system

  netstat -ntlp|grep 30001
  
  14.4.在Firefox浏览器输入Dashboard访问地址：https://10.10.10.10:30001
  14.5.查看访问Dashboard的认证令牌
  kubectl create serviceaccount  dashboard-admin -n kube-system
  kubectl create clusterrolebinding  dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin
  kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk '/dashboard-admin/{print $1}')
  14.6.使用输出的token登录Dashboard。
   eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tZ2M0cmsiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYjQ3MzE3NjktOTc2Ni0xMWU5LWE4NWQtMDAxNjNlMGFmNDkzIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.eOZ-5d7icF_OPTk2L9mYrKnSqfKpK2bgd4uu3sYV2Iz95y1bOF8etXItre_baUW5-AOU22Evfs-B7ISKCnx1X2eq6pQa9jcSP5xZxfOE--S2V_kTD1_P1oJf6p6jEvD40Gl7bgk8kgvm1eMd9SqwM9CXHVqA3C8grmefuoqNBRNLTAneM-pizBMCKm32ducviWi2RuCGSotkG0F8plC_8UuxRma6sPH1s04BpCGzGU564iI3obmxbKYxR8JVUu_ROABTO0cDTXHUCXwtIk1xDv6i-UzAZYd9MrGNnNoOIADtfmRdYgeOTl39NmrlmsUFKzwJsGNDXZgYGTYeCcTvZg

  安装完成！




echo "cluster.name: es-cluster
node.name: es-master
network.bind_host: 0.0.0.0
network.publish_host: 114.67.80.169
http.port: 9201
transport.tcp.port: 9301
http.cors.enabled: true
http.cors.allow-origin: \"*\"
node.master: true
node.data: true
discovery.zen.ping.unicast.hosts: [\"114.67.80.169:9302\",\"114.67.80.169:9303\"]
discovery.zen.minimum_master_nodes: 1" > /root/elasticsearch-cluster-yml/es-cluster1.yml


echo "cluster.name: es-cluster
node.name: es-data2
network.bind_host: 0.0.0.0
network.publish_host: 114.67.80.169
http.port: 9203
transport.tcp.port: 9303
http.cors.enabled: true
http.cors.allow-origin: \"*\"
node.master: false
node.data: true
discovery.zen.ping.unicast.hosts: [\"114.67.80.169:9301\",\"114.67.80.169:9302\"]
discovery.zen.minimum_master_nodes: 1" > /root/elasticsearch-cluster-yml/es-cluster3.yml


